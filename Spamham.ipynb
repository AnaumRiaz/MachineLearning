{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainining number:  400\n",
      "returning nothing\n",
      "returning nothing\n",
      "trainining number:  2041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "More than one match found for (?:with(?! cipher)\\s+(?P<with>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+id|\\s+for|\\s+via|;)) in from unknown HELO ?192.168.0.100? salimma1@212.18.241.211 with plain by smtp.mail.vip.sc5.yahoo.com with SMTP; 10 Oct 2002 10:30:25 -0000\n",
      "More than one match found for (?:with(?! cipher)\\s+(?P<with>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+id|\\s+for|\\s+via|;)) in from p977.as2.cra.dublin.eircom.net HELO mfrenchw2k mfrench42@159.134.179.209 with login by smtp.mail.vip.sc5.yahoo.com with SMTP; 22 Aug 2002 22:02:25 -0000\n",
      "More than one match found for (?:id\\s+(?P<id>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+with(?! cipher)|\\s+for|\\s+via|;)) in from chassid 4.65.20.230 by out001.verizon.net InterMail vM.5.01.05.09 201-253-122-126-109-20020611 with ESMTP id <20021005011206.OGMC3265.out001.verizon.net@chassid> for <rpm-list@freshrpms.net>; Fri, 4 Oct 2002 20:12:06 -0500\n",
      "More than one match found for (?:id\\s+(?P<id>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+with(?! cipher)|\\s+for|\\s+via|;)) in from chassid 4.65.20.230 by out002.verizon.net InterMail vM.5.01.05.09 201-253-122-126-109-20020611 with ESMTP id <20021005195843.LSGV2867.out002.verizon.net@chassid> for <rpm-list@freshrpms.net>; Sat, 5 Oct 2002 14:58:43 -0500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree\n",
    "import mailbox   \n",
    "import lxml.html\n",
    "import lxml.html.clean\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import PorterStemmer\n",
    "import email\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Import the email modules we'll need\n",
    "from email.parser import Parser\n",
    "import mailparser as mp\n",
    "from email.mime.text import MIMEText\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# path = spam_path\n",
    "# i = 0\n",
    "# for filename in os.listdir(path):\n",
    "#     print('orig', filename)\n",
    "#     os.rename(os.path.join(path,filename), os.path.join(path,'mail_'+str(i)))\n",
    "#     print('changed', filename)\n",
    "#     i = i +1\n",
    "# print(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class preparing_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x, y=None):\n",
    "        #print('type of x is' , type(x))\n",
    "        #x = x.lower()\n",
    "        m_body, m_headers, sub = dividing_mail(x)\n",
    "        if m_body != '':\n",
    "            m_body = m_body.lower()\n",
    "            #m_body = m_body.split('.').join(\"\");\n",
    "            urls = counting_urls(m_body)\n",
    "            m_body = cleaning_htmlent(m_body)\n",
    "            \n",
    "            name_sender, dom_sender = x.from_[0][0], x.from_[0][1].split('@')[-1]\n",
    "        \n",
    "            sub_list = keep_only_important_words(sub)\n",
    "            body_list = keep_only_important_words(m_body)\n",
    "            sub_list = removing_alphas(sub_list)\n",
    "            body_list = removing_alphas(body_list)\n",
    "            \n",
    "            #save words record?\n",
    "            ps = PorterStemmer()\n",
    "            s_c =   [ps.stem(w) for w in sub_list]\n",
    "            b_c =   [ps.stem(w) for w in body_list]\n",
    "            b_c.extend([urls, name_sender, dom_sender])\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print('returning nothing')\n",
    "            s_c =['','','']\n",
    "            b_c = ['0','','']\n",
    "        return s_c, b_c\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def keep_only_important_words(data_txt):\n",
    "        m_words =[]\n",
    "        for word, pos in nltk.pos_tag(TreebankWordTokenizer().tokenize(data_txt)):\n",
    "            #if pos not in ['IN', '.', 'CD', 'DT', 'RB', 'VBP', 'TO', 'PRP', 'C']: VB and VBP removed\n",
    "            if pos in ['NN',  'JJ', 'VBN'] :\n",
    "                m_words.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        return(m_words)\n",
    "\n",
    "def cleaning_htmlent(text):\n",
    "    if text != '':\n",
    "        doc = lxml.html.fromstring(text)\n",
    "        cleaner = lxml.html.clean.Cleaner(style=True)\n",
    "        doc = cleaner.clean_html(doc)\n",
    "        content = doc.text_content()\n",
    "        return content\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def counting_urls(text):\n",
    "#     soup = BeautifulSoup(text, 'html.parser')\n",
    "#     num = soup.find_all('a')\n",
    "#     print('number is ', (num))\n",
    "\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    num_urls = len(urls)\n",
    "    return num_urls\n",
    "  \n",
    "def dividing_mail(mail):\n",
    "        m_body = mail.body\n",
    "        m_headers = mail.headers\n",
    "        \n",
    "        sub = mail.subject\n",
    "        return m_body, m_headers, sub\n",
    "\n",
    "def removing_alphas(doc):\n",
    "    dellist = []\n",
    "    for i in range(len(doc)):\n",
    "        if not doc[i].isalpha():\n",
    "            dellist.append(i)\n",
    "       \n",
    "    for index in sorted(dellist, reverse=True):\n",
    "        del doc[index]\n",
    "    return doc\n",
    "    \n",
    "    \n",
    "\n",
    "def extracting_feats(body_list, sub_list, arr):\n",
    "    i = 0\n",
    "    fl = []\n",
    "    for  l in list([body_list, sub_list]):\n",
    "        flatten_list = [a for t in l for a in t]\n",
    "        out = Counter(flatten_list)\n",
    "        out = out.most_common(arr[i])\n",
    "        i = i + 1 \n",
    "        cols = [m[0] for m in out]\n",
    "        fl.append(cols)\n",
    "    return fl\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "sub_list = []\n",
    "body_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Giving filepaths for the data\n",
    "dat_path = \"datasets/spamham\"\n",
    "spam_path = dat_path + '/easy_spam'\n",
    "ham_path = dat_path + '/easy_ham'\n",
    "len_spam = len(next(os.walk(spam_path))[2])\n",
    "len_ham = len(next(os.walk(ham_path))[2])\n",
    "ar_dict = [(spam_path, len_spam), (ham_path, len_ham)] \n",
    "sl = math.ceil(0.8 * len_spam)\n",
    "hl = math.ceil(0.8 * len_ham)\n",
    "\n",
    "y_train = [0] * sl + [1] * hl\n",
    "n_list = []\n",
    "    \n",
    "for sp, num in ar_dict:\n",
    "    tr_n = math.ceil(0.8 * int(num))\n",
    "    print('trainining number: ', tr_n)\n",
    "    \n",
    "    for file_num in range(tr_n):\n",
    "        #Reading an email and getting the content\n",
    "        data_mail = sp + '/mail_' + str(file_num)\n",
    "        #print(data_mail)\n",
    "        \n",
    "        \n",
    "        mail = mp.parse_from_file(data_mail)\n",
    "        obj = preparing_data()\n",
    "        s, b = obj.fit_transform(mail)\n",
    "        n_list.append(b[-3:])\n",
    "        sub_list.append(s)\n",
    "        body_list.append(b[:-3])\n",
    "        \n",
    "            \n",
    "\n",
    "featurevectors = extracting_feats(body_list[:len_spam], sub_list[:len_spam], [200 ,50])\n",
    "featurevectorh = extracting_feats(body_list[len_spam:], sub_list[len_spam:], [300 ,50])\n",
    "#feat_vals = set(featurevectors) + set(featurevectorh)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164357\n"
     ]
    }
   ],
   "source": [
    "bodycols = set(featurevectors[0] + featurevectorh[0])\n",
    "subcols = set(featurevectors[1] + featurevectorh[1])\n",
    "\n",
    "\n",
    "sample_value = []\n",
    "\n",
    "\n",
    "for num in range(hl + sl):\n",
    "    for feature in bodycols:\n",
    "        #print(feature)\n",
    "        if feature in body_list[num]:\n",
    "            sample_value.append(body_list[num].count(feature))\n",
    "            \n",
    "        else:\n",
    "            sample_value.append(0)\n",
    "                #print(sample_value)\n",
    "    for feature in subcols:\n",
    "        #print(len(sample_value))\n",
    "        if feature in sub_list[num]:\n",
    "            sample_value.append(sub_list[num].count(feature))\n",
    "                #print(sample_value)\n",
    "        else:\n",
    "            sample_value.append(0)\n",
    "            #print(sample_value)\n",
    "    urlno = n_list[num][0]\n",
    "    sample_value.append(urlno)\n",
    "            \n",
    "   \n",
    "print(len(sample_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2441, 477)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1      875\n",
       "2      539\n",
       "0      434\n",
       "3      268\n",
       "4      136\n",
       "6       37\n",
       "5       32\n",
       "7       18\n",
       "8       15\n",
       "9       10\n",
       "23       8\n",
       "19       7\n",
       "12       7\n",
       "11       5\n",
       "17       5\n",
       "15       4\n",
       "13       4\n",
       "16       4\n",
       "24       4\n",
       "10       4\n",
       "27       3\n",
       "20       3\n",
       "26       3\n",
       "14       2\n",
       "28       2\n",
       "21       2\n",
       "39       2\n",
       "22       1\n",
       "42       1\n",
       "25       1\n",
       "45       1\n",
       "52       1\n",
       "137      1\n",
       "44       1\n",
       "37       1\n",
       "Name: URL_num, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.reshape(sample_value, (2441, 477)) \n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "colnames =  list(bodycols) + list(subcols) + ['URL_num']\n",
    "\n",
    "df = pd.DataFrame(data = data, columns = colnames)\n",
    "df['URL_num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99141104 0.98400984 0.97662977]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "\n",
    "result = cross_val_score(clf, df.values, y_train, cv = 3, scoring = \"accuracy\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [('a', 1), ('b', 4), ('e', 3)]\n",
    "d[0] \n",
    "m_words =[]\n",
    "data_txt = 'i am and happy your'\n",
    "for word, pos in nltk.pos_tag(TreebankWordTokenizer().tokenize(data_txt)):\n",
    "    if pos not in ['IN', '.', 'CD', 'DT', 'RB', 'VBP']:\n",
    "        print (word, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_list[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
