{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "import xml.etree.ElementTree\n",
    "import lxml.html\n",
    "import lxml.html.clean\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import mailparser as mp\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#to clean mails\n",
    "def dividing_mail(mail):\n",
    "        m_body = mail.body\n",
    "        m_headers = mail.headers\n",
    "        sub = mail.subject\n",
    "        return m_body, m_headers, sub\n",
    "\n",
    "def counting_urls(text):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    num_urls = len(urls)\n",
    "    return num_urls\n",
    "\n",
    "def cleaning_htmlent(text):\n",
    "    if text != '':\n",
    "        doc = lxml.html.fromstring(text)\n",
    "        cleaner = lxml.html.clean.Cleaner(style=True)\n",
    "        doc = cleaner.clean_html(doc)\n",
    "        content = doc.text_content()\n",
    "        return content\n",
    "    else:\n",
    "        return []\n",
    "   \n",
    "    \n",
    "def removing_alphas(doc):\n",
    "    dellist = []\n",
    "    for i in range(len(doc)):\n",
    "        if not doc[i].isalpha():\n",
    "            dellist.append(i)\n",
    "       \n",
    "    for index in sorted(dellist, reverse=True):\n",
    "        del doc[index]\n",
    "    return doc\n",
    "    \n",
    "def keep_only_important_words(data_txt):\n",
    "        m_words =[]\n",
    "        for word, pos in pos_tag(TreebankWordTokenizer().tokenize(data_txt)):\n",
    "            #if pos not in ['IN', '.', 'CD', 'DT', 'RB', 'VBP', 'TO', 'PRP', 'C']: VB and VBP removed\n",
    "            if pos in ['NN',  'JJ', 'VBN'] :\n",
    "                m_words.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        return(m_words)\n",
    "\n",
    "    \n",
    "#getting most frequent words from body and subject lists\n",
    "def extracting_feats(body_list, sub_list, arr):\n",
    "    i = 0\n",
    "    fl = []\n",
    "    for  l in list([body_list, sub_list]):\n",
    "        flatten_list = [a for t in l for a in t]\n",
    "        out = Counter(flatten_list)\n",
    "        out = out.most_common(arr[i])\n",
    "        i = i + 1 \n",
    "        cols = [m[0] for m in out]\n",
    "        fl.append(cols)\n",
    "    return fl\n",
    "      \n",
    "\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data for  datasets/spamham/easy_spam\n",
      "preparing data for  datasets/spamham/easy_ham\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "More than one match found for (?:with(?! cipher)\\s+(?P<with>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+id|\\s+for|\\s+via|;)) in from unknown HELO ?192.168.0.100? salimma1@212.18.241.211 with plain by smtp.mail.vip.sc5.yahoo.com with SMTP; 10 Oct 2002 10:30:25 -0000\n",
      "More than one match found for (?:with(?! cipher)\\s+(?P<with>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+id|\\s+for|\\s+via|;)) in from p977.as2.cra.dublin.eircom.net HELO mfrenchw2k mfrench42@159.134.179.209 with login by smtp.mail.vip.sc5.yahoo.com with SMTP; 22 Aug 2002 22:02:25 -0000\n",
      "More than one match found for (?:id\\s+(?P<id>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+with(?! cipher)|\\s+for|\\s+via|;)) in from chassid 4.65.20.230 by out001.verizon.net InterMail vM.5.01.05.09 201-253-122-126-109-20020611 with ESMTP id <20021005011206.OGMC3265.out001.verizon.net@chassid> for <rpm-list@freshrpms.net>; Fri, 4 Oct 2002 20:12:06 -0500\n",
      "More than one match found for (?:id\\s+(?P<id>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+with(?! cipher)|\\s+for|\\s+via|;)) in from chassid 4.65.20.230 by out002.verizon.net InterMail vM.5.01.05.09 201-253-122-126-109-20020611 with ESMTP id <20021005195843.LSGV2867.out002.verizon.net@chassid> for <rpm-list@freshrpms.net>; Sat, 5 Oct 2002 14:58:43 -0500\n",
      "Unable to match any clauses in qfscan 0.5. spam=2/0/0/0 remoteip=172.16.0.211 : 29 Aug 2002 03:27:47 -0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making features to use ... \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#transformer for applying diff functions to clean up the mail and get list of words\n",
    "class preparing_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x, y=None):\n",
    "        #print('type of x is' , type(x))\n",
    "        #x = x.lower()\n",
    "        m_body, m_headers, sub = dividing_mail(x)\n",
    "        if m_body != '':\n",
    "            m_body = m_body.lower()\n",
    "            urls = counting_urls(m_body)\n",
    "            m_body = cleaning_htmlent(m_body)\n",
    "            \n",
    "            name_sender, dom_sender = x.from_[0][0], x.from_[0][1].split('@')[-1]\n",
    "        \n",
    "            sub_list = keep_only_important_words(sub)\n",
    "            body_list = keep_only_important_words(m_body)\n",
    "            sub_list = removing_alphas(sub_list)\n",
    "            body_list = removing_alphas(body_list)\n",
    "            \n",
    "            ps = PorterStemmer()\n",
    "            s_c =   [ps.stem(w) for w in sub_list]\n",
    "            b_c =   [ps.stem(w) for w in body_list]\n",
    "            b_c.extend([urls, name_sender, dom_sender])\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            s_c =['','','']\n",
    "            b_c = ['0','','']\n",
    "        return s_c, b_c\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Giving filepaths for the data\n",
    "dat_path = \"datasets/spamham\"\n",
    "spam_path = dat_path + '/easy_spam'\n",
    "ham_path = dat_path + '/easy_ham'\n",
    "\n",
    "len_spam = len(next(os.walk(spam_path))[2])\n",
    "len_ham = len(next(os.walk(ham_path))[2])\n",
    "\n",
    "ar_dict = [(spam_path, len_spam), (ham_path, len_ham)] \n",
    "sl = math.ceil(0.8 * len_spam)\n",
    "hl = math.ceil(0.8 * len_ham)\n",
    "\n",
    "y_train = [0] * sl + [1] * hl\n",
    "y_test = [0] * (len_spam - sl) + [1] * (len_ham - hl)\n",
    "\n",
    "n_list = []\n",
    "sub_list = []\n",
    "body_list = []\n",
    "\n",
    "#Reading emails and transforming them in a list of words and combining them for all mails\n",
    "for sp, num in ar_dict:\n",
    "    print('preparing data for ', sp)\n",
    "    for file_num in range(num):\n",
    "        #Reading an email and getting the content\n",
    "        data_mail = sp + '/mail_' + str(file_num)\n",
    "        \n",
    "        mail = mp.parse_from_file(data_mail)\n",
    "        obj = preparing_data()\n",
    "        s, b = obj.fit_transform(mail)\n",
    "        n_list.append(b[-3:])\n",
    "        sub_list.append(s)\n",
    "        body_list.append(b[:-3])\n",
    "        \n",
    "#dividing training and testing for all mails            \n",
    "train_body = body_list[:sl] + body_list[len_spam: len_spam + hl]\n",
    "test_body = body_list[sl:len_spam] + body_list[len_spam + hl:]\n",
    "train_sub = sub_list[:sl] + sub_list[len_spam:len_spam + hl]\n",
    "test_sub = sub_list[sl:len_spam] + sub_list[len_spam + hl:]\n",
    "n_list_train = n_list[:sl] + n_list[len_spam: len_spam + hl]\n",
    "n_list_test = n_list[sl:len_spam] + n_list[len_spam + hl:]\n",
    "\n",
    "#getting all important features to use for classification\n",
    "print('Making features to use ... ')\n",
    "featurevectors = extracting_feats(train_body[:sl], train_sub[:sl], [200 ,50])\n",
    "featurevectorh = extracting_feats(train_body[sl:], train_sub[sl:], [300 ,50])\n",
    "print('Done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training dataframe:  (2441, 485)\n",
      "shape of testing dataframe:  (610, 485)\n"
     ]
    }
   ],
   "source": [
    "#making sure same columns are not repeated\n",
    "bodycols = set(featurevectors[0] + featurevectorh[0])\n",
    "subcols = set(featurevectors[1] + featurevectorh[1])\n",
    "colnames =  list(bodycols) + list(subcols) + ['URL_num']\n",
    "numtoreshape = len(colnames)\n",
    "totaldata = hl + sl\n",
    "\n",
    "#making feature vectors for all mails, training and testing and makin dataframes        \n",
    "def feature_vectors(d_length, blist, slist, nlist):\n",
    "    sample_value = []\n",
    "    for num in range(d_length):\n",
    "        for feature in bodycols:\n",
    "            if feature in blist[num]:\n",
    "                sample_value.append(blist[num].count(feature))\n",
    "            else:\n",
    "                sample_value.append(0)\n",
    "\n",
    "        for feature in subcols:\n",
    "            if feature in slist[num]:\n",
    "                sample_value.append(slist[num].count(feature))\n",
    "            else:\n",
    "                sample_value.append(0)\n",
    "    \n",
    "        urlno = nlist[num][0]\n",
    "        sample_value.append(urlno)\n",
    "\n",
    "    #reshaping data to convert it in dataframe\n",
    "    data = np.reshape(sample_value, (-1, numtoreshape)) \n",
    "            \n",
    "    df = pd.DataFrame(data = data, columns = colnames)\n",
    "    return df   \n",
    "        \n",
    "df = feature_vectors(totaldata, train_body, train_sub, n_list_train)       \n",
    "df_test = feature_vectors(len(test_body), test_body, test_sub, n_list_test)\n",
    "\n",
    "print('shape of training dataframe: ', df.shape)\n",
    "print('shape of testing dataframe: ', df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores for best estimator \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99018405, 0.9901599 , 0.9704797 ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42) \n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_estimators': randint(10, 50),\n",
    "     'max_features': randint(1, numtoreshape),\n",
    "}\n",
    "\n",
    "newgridsearch = RandomizedSearchCV(clf, param_distributions = params, n_iter=20, cv = 5, scoring='accuracy')\n",
    "r_grid_search = newgridsearch.fit(df, y_train)\n",
    "\n",
    "final_model = r_grid_search.best_estimator_\n",
    "frst_scrs = cross_val_score(final_model, df, y_train, cv = 3, scoring = \"accuracy\")\n",
    "print('Training scores for best estimator ')\n",
    "frst_scrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scores for best estimator \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9868852459016394"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = final_model.predict(df_test)\n",
    "accuracy = sum(1 for x,y in zip(predictions,y_test) if x == y) / len(predictions)\n",
    "print('Testing scores for best estimator ')\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score = (y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
