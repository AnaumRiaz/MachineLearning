{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainining number:  400\n",
      "returning nothing\n",
      "returning nothing\n",
      "trainining number:  2041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "More than one match found for (?:with(?! cipher)\\s+(?P<with>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+id|\\s+for|\\s+via|;)) in from unknown HELO ?192.168.0.100? salimma1@212.18.241.211 with plain by smtp.mail.vip.sc5.yahoo.com with SMTP; 10 Oct 2002 10:30:25 -0000\n",
      "More than one match found for (?:with(?! cipher)\\s+(?P<with>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+id|\\s+for|\\s+via|;)) in from p977.as2.cra.dublin.eircom.net HELO mfrenchw2k mfrench42@159.134.179.209 with login by smtp.mail.vip.sc5.yahoo.com with SMTP; 22 Aug 2002 22:02:25 -0000\n",
      "More than one match found for (?:id\\s+(?P<id>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+with(?! cipher)|\\s+for|\\s+via|;)) in from chassid 4.65.20.230 by out001.verizon.net InterMail vM.5.01.05.09 201-253-122-126-109-20020611 with ESMTP id <20021005011206.OGMC3265.out001.verizon.net@chassid> for <rpm-list@freshrpms.net>; Fri, 4 Oct 2002 20:12:06 -0500\n",
      "More than one match found for (?:id\\s+(?P<id>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+with(?! cipher)|\\s+for|\\s+via|;)) in from chassid 4.65.20.230 by out002.verizon.net InterMail vM.5.01.05.09 201-253-122-126-109-20020611 with ESMTP id <20021005195843.LSGV2867.out002.verizon.net@chassid> for <rpm-list@freshrpms.net>; Sat, 5 Oct 2002 14:58:43 -0500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sub is  2441\n",
      "length of body is  2441\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree\n",
    "import mailbox   \n",
    "import lxml.html\n",
    "import lxml.html.clean\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import PorterStemmer\n",
    "import email\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Import the email modules we'll need\n",
    "from email.parser import Parser\n",
    "import mailparser as mp\n",
    "from email.mime.text import MIMEText\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# path = spam_path\n",
    "# i = 0\n",
    "# for filename in os.listdir(path):\n",
    "#     print('orig', filename)\n",
    "#     os.rename(os.path.join(path,filename), os.path.join(path,'mail_'+str(i)))\n",
    "#     print('changed', filename)\n",
    "#     i = i +1\n",
    "# print(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class preparing_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x, y=None):\n",
    "        #print('type of x is' , type(x))\n",
    "        #x = x.lower()\n",
    "        m_body, m_headers, sub = dividing_mail(x)\n",
    "        if m_body != '':\n",
    "            m_body = m_body.lower()\n",
    "            #m_body = m_body.split('.').join(\"\");\n",
    "            urls = counting_urls(m_body)\n",
    "            m_body = cleaning_htmlent(m_body)\n",
    "            \n",
    "            #name_sender, dom_sender = x.from_[0][0], x.from_[0][1].split('@')[-1]\n",
    "            dom_sender = x.from_[0][1].split('@')[-1]\n",
    "        \n",
    "            sub_list = keep_only_important_words(sub)\n",
    "            body_list = keep_only_important_words(m_body)\n",
    "            sub_list = removing_alphas(sub_list)\n",
    "            body_list = removing_alphas(body_list)\n",
    "            \n",
    "            ps = PorterStemmer()\n",
    "            s_c =   [ps.stem(w) for w in sub_list]\n",
    "            b_c =   [ps.stem(w) for w in body_list]\n",
    "            b_c.extend([urls, dom_sender])\n",
    "        else:\n",
    "            print('returning nothing')\n",
    "            s_c =[]\n",
    "            b_c = []\n",
    "        return s_c, b_c\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def keep_only_important_words(data_txt):\n",
    "        m_words =[]\n",
    "        for word, pos in nltk.pos_tag(TreebankWordTokenizer().tokenize(data_txt)):\n",
    "            #if pos not in ['IN', '.', 'CD', 'DT', 'RB', 'VBP', 'TO', 'PRP', 'C']: VB and VBP removed\n",
    "            if pos in ['NN',  'JJ', 'VBN'] :\n",
    "                m_words.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        return(m_words)\n",
    "\n",
    "def cleaning_htmlent(text):\n",
    "    if text != '':\n",
    "        doc = lxml.html.fromstring(text)\n",
    "        cleaner = lxml.html.clean.Cleaner(style=True)\n",
    "        doc = cleaner.clean_html(doc)\n",
    "        content = doc.text_content()\n",
    "        return content\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def counting_urls(text):\n",
    "#     soup = BeautifulSoup(text, 'html.parser')\n",
    "#     num = soup.find_all('a')\n",
    "#     print('number is ', (num))\n",
    "\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    num_urls = len(urls)\n",
    "    return num_urls\n",
    "  \n",
    "def dividing_mail(mail):\n",
    "        m_body = mail.body\n",
    "        m_headers = mail.headers\n",
    "        \n",
    "        sub = mail.subject\n",
    "        return m_body, m_headers, sub\n",
    "\n",
    "sub_list = []\n",
    "body_list = []\n",
    "\n",
    "\n",
    "\n",
    "def removing_alphas(doc):\n",
    "    dellist = []\n",
    "    for i in range(len(doc)):\n",
    "        if not doc[i].isalpha():\n",
    "            dellist.append(i)\n",
    "       \n",
    "    for index in sorted(dellist, reverse=True):\n",
    "        del doc[index]\n",
    "    return doc\n",
    "    \n",
    "\n",
    "#Giving filepaths for the data\n",
    "dat_path = \"datasets/spamham\"\n",
    "spam_path = dat_path + '/easy_spam'\n",
    "ham_path = dat_path + '/easy_ham'\n",
    "len_spam = len(next(os.walk(spam_path))[2])\n",
    "len_ham = len(next(os.walk(ham_path))[2])\n",
    "ar_dict = [(spam_path, len_spam), (ham_path, len_ham)] \n",
    "\n",
    "y_train = [0] * math.ceil(0.8 * len_spam) + [1] * math.ceil(0.8 * len_spam)\n",
    "n_list = []\n",
    "    \n",
    "for sp, num in ar_dict:\n",
    "    tr_n = math.ceil(0.8 * int(num))\n",
    "    print('trainining number: ', tr_n)\n",
    "    \n",
    "    for file_num in range(tr_n):\n",
    "        #Reading an email and getting the content\n",
    "        data_mail = sp + '/mail_' + str(file_num)\n",
    "        #print(data_mail)\n",
    "        \n",
    "        \n",
    "        mail = mp.parse_from_file(data_mail)\n",
    "        obj = preparing_data()\n",
    "        s, b = obj.fit_transform(mail)\n",
    "        n_list.append(b[-2:])\n",
    "        sub_list.append(s)\n",
    "        body_list.append(b)\n",
    "\n",
    "\n",
    "def extracting_common(body_list, sub_list, arr):\n",
    "    i = 0\n",
    "    fl = []\n",
    "    for  l in list([body_list, sub_list]):\n",
    "        flatten_list = [a for t in l for a in t]\n",
    "        out = Counter(flatten_list)\n",
    "        out = out.most_common(arr[i])\n",
    "        i = i + 1 \n",
    "        fl.append(out)\n",
    "    #out = [(k, v) for k, v in out.items() if v >= to_choose]\n",
    "    #l_words = [tup[0] for tup in out]\n",
    "    return out\n",
    "      \n",
    "        #words_mails.append(l)\n",
    "        #print(len(words_mails))\n",
    "print('length of sub is ', len(sub_list))\n",
    "print('length of body is ', len(body_list))\n",
    "        \n",
    "# b_fvs = extracting_common(body_list[:len_spam], 200)\n",
    "# s_fvs = extracting_common(sub_list[:len_spam], 50)\n",
    "# featurevectors = b_fvs + s_fvs #+ n_list[:len_spam]\n",
    "featurevectorss = extracting_common(body_list[:len_spam], sub_list[:len_spam], [200 ,50])\n",
    "featurevectorss = extracting_common(body_list[:len_spam], sub_list[:len_spam], [300 ,50])\n",
    "\n",
    "# b_fvh = extracting_common(body_list[len_spam:], 300)\n",
    "# s_fvh = extracting_common(sub_list[len_spam:], 50)\n",
    "# featurevectorh = b_fvh + s_fvh #+ n_list[len_spam:]\n",
    "\n",
    "\n",
    "print(len(featurevectors))\n",
    "#print(len(featurevectorh))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 2]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_list\n",
    "i = 500\n",
    "#for t in n_list:\n",
    "    #    if t:\n",
    "        #print(i)\n",
    "        #print(t[0])\n",
    "#    i = i +1\n",
    "l = []\n",
    "fl1 = [1,2,3]\n",
    "out = Counter(fl1)\n",
    "\n",
    "fl2 = [4,5,6]\n",
    "fl2.append(2)\n",
    "fl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_words =[]\n",
    "data_txt = 'i am and happy your'\n",
    "for word, pos in nltk.pos_tag(TreebankWordTokenizer().tokenize(data_txt)):\n",
    "    if pos not in ['IN', '.', 'CD', 'DT', 'RB', 'VBP']:\n",
    "        print (word, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = re.findall('anau', 'anadu is an  adf  eargf er erg reg ret')\n",
    "num_urls = len(urls)\n",
    "num_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
