{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "import xml.etree.ElementTree\n",
    "import lxml.html\n",
    "import lxml.html.clean\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import mailparser as mp\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#-------------------to clean mails----------------------------\n",
    "def dividing_mail(mail):\n",
    "        m_body = mail.body\n",
    "        m_headers = mail.headers\n",
    "        sub = mail.subject\n",
    "        return m_body, m_headers, sub\n",
    "\n",
    "def counting_urls(text):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    num_urls = len(urls)\n",
    "    return num_urls\n",
    "\n",
    "def cleaning_htmlent(text):\n",
    "    if text != '':\n",
    "        doc = lxml.html.fromstring(text)\n",
    "        cleaner = lxml.html.clean.Cleaner(style=True)\n",
    "        doc = cleaner.clean_html(doc)\n",
    "        content = doc.text_content()\n",
    "        return content\n",
    "    else:\n",
    "        return []\n",
    "   \n",
    "    \n",
    "def removing_alphas(doc):\n",
    "    dellist = []\n",
    "    for i in range(len(doc)):\n",
    "        if not doc[i].isalpha():\n",
    "            dellist.append(i)\n",
    "       \n",
    "    for index in sorted(dellist, reverse=True):\n",
    "        del doc[index]\n",
    "    return doc\n",
    "    \n",
    "def keep_only_important_words(data_txt):\n",
    "        m_words =[]\n",
    "        for word, pos in pos_tag(TreebankWordTokenizer().tokenize(data_txt)):\n",
    "            #if pos not in ['IN', '.', 'CD', 'DT', 'RB', 'VBP', 'TO', 'PRP', 'C']: VB and VBP removed\n",
    "            if pos in ['NN',  'JJ', 'VBN'] :\n",
    "                m_words.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        return(m_words)\n",
    "\n",
    "    \n",
    "#-----------getting most frequent words from body and subject lists to make features---------\n",
    "def extracting_feats(body_list, sub_list, arr):\n",
    "    i = 0\n",
    "    fl = []\n",
    "    for  l in list([body_list, sub_list]):\n",
    "        flatten_list = [a for t in l for a in t]\n",
    "        out = Counter(flatten_list)\n",
    "        out = out.most_common(arr[i])\n",
    "        i = i + 1 \n",
    "        cols = [m[0] for m in out]\n",
    "        fl.append(cols)\n",
    "    return fl\n",
    "      \n",
    "\n",
    "\n",
    "#---------making feature vectors & Df for all mails, training and testing(used by transformer--------       \n",
    "def feature_vectors(blist, slist, nlist, colnames, bodycols, subcols):\n",
    "    sample_value = []\n",
    "    numtoreshape = len(colnames)\n",
    "    \n",
    "    for num in range(len(blist)):\n",
    "        for feature in bodycols:\n",
    "            if feature in blist[num]:\n",
    "                sample_value.append(blist[num].count(feature))\n",
    "            else:\n",
    "                sample_value.append(0)\n",
    "\n",
    "        for feature in subcols:\n",
    "            if feature in slist[num]:\n",
    "                sample_value.append(slist[num].count(feature))\n",
    "            else:\n",
    "                sample_value.append(0)\n",
    "    \n",
    "        urlno = nlist[num][0]\n",
    "        sample_value.append(urlno)\n",
    "\n",
    "    #reshaping data to convert it in dataframe\n",
    "    data = np.reshape(sample_value, (-1, numtoreshape)) \n",
    "            \n",
    "    df = pd.DataFrame(data = data, columns = colnames)\n",
    "    return df   \n",
    "\n",
    "           \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#-------------All the transformers-------------------------------\n",
    "\n",
    "\n",
    "#takes a list of mails object and return a list of words for all the mails\n",
    "class preparing_data(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, convertlower = True, counturl = True, senderinfo = True):\n",
    "        self.convertlower = convertlower\n",
    "        self.counturl = counturl\n",
    "        self.senderinfo = senderinfo\n",
    "        pass\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x, y=None):    \n",
    "        print('Preparing data...')\n",
    "        list_words = []\n",
    "        for filenum in range(len(x)):\n",
    "            m_body, m_headers, sub = dividing_mail(x[filenum])\n",
    "            if m_body != '':\n",
    "                if self.convertlower == True:\n",
    "                    m_body = m_body.lower()\n",
    "                if self.counturl == True:\n",
    "                    urls = counting_urls(m_body)\n",
    "                else:\n",
    "                    return 0\n",
    "            \n",
    "                m_body = cleaning_htmlent(m_body)\n",
    "                if self.senderinfo == True:\n",
    "                    name_sender, dom_sender = x[filenum].from_[0][0], x[filenum].from_[0][1].split('@')[-1]\n",
    "        \n",
    "                sub_list = keep_only_important_words(sub)\n",
    "                body_list = keep_only_important_words(m_body)\n",
    "                sub_list = removing_alphas(sub_list)\n",
    "                body_list = removing_alphas(body_list)\n",
    "            \n",
    "                ps = PorterStemmer()\n",
    "                s_c =   [ps.stem(w) for w in sub_list]\n",
    "                b_c =   [ps.stem(w) for w in body_list]\n",
    "                b_c.extend([urls, name_sender, dom_sender])\n",
    "                tojoin = [b_c, s_c]\n",
    "                \n",
    "            else:\n",
    "                s_c =['','','']\n",
    "                b_c = ['0','','']\n",
    "                tojoin = [b_c, s_c]\n",
    "            list_words.append(tojoin)\n",
    "        return list_words\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#transformer for getting the features from the lists of words\n",
    "class making_features(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_f_bodyham = 300 , n_f_bodyspam = 200, n_f_subham = 50, n_f_subspam = 50, cols =[]):\n",
    "        self.n_f_bodyham = n_f_bodyham\n",
    "        self.n_f_bodyspam = n_f_bodyspam\n",
    "        self.n_f_subham = n_f_subham\n",
    "        self.n_f_subspam = n_f_subspam \n",
    "        self.cols = cols\n",
    "        self.bodycols = cols\n",
    "        self.subcols = cols\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        \n",
    "        b_lists = [onelist[0][:-3] for onelist in x]\n",
    "        s_lists = [onelist[1] for onelist in x]\n",
    "        n_lists = [onelist[0][-3:] for onelist in x]\n",
    "        \n",
    "        if (len(x) > 2400):\n",
    "            print('making features, inside training')\n",
    "            featurevectors = extracting_feats(b_lists[:sl], s_lists[:sl], [self.n_f_bodyspam ,self.n_f_subspam])\n",
    "            featurevectorh = extracting_feats(b_lists[sl:], s_lists[sl:], [self.n_f_bodyham ,self.n_f_subham])\n",
    "            \n",
    "            bodycols = set(featurevectors[0] + featurevectorh[0])\n",
    "            subcols = set(featurevectors[1] + featurevectorh[1])\n",
    "            colnames =  list(bodycols) + list(subcols) + ['URL_num']   \n",
    "            \n",
    "            self.cols = colnames\n",
    "            self.bodycols = bodycols\n",
    "            self.subcols = subcols\n",
    "            \n",
    "            \n",
    "            return [b_lists] + [s_lists] + [n_lists] + [bodycols] + [subcols] + [colnames] \n",
    "        else:\n",
    "            print('making features, inside testing')\n",
    "            return [b_lists] + [s_lists] + [n_lists] + [self.bodycols] + [self.subcols] + [self.cols] \n",
    "        \n",
    "        \n",
    "#Makes the featurevectors and dataframe by using the function feature_vectors        \n",
    "class making_fvectors(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        print('preparing feature vectors... ')\n",
    "        bodycols = x[3]\n",
    "        subcols = x[4]\n",
    "        cols = x[-1]\n",
    "        df = feature_vectors( x[0], x[1], x[2], cols, bodycols, subcols)\n",
    "        return df\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data for  datasets/spamham/easy_spam\n",
      "preparing data for  datasets/spamham/easy_ham\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "More than one match found for (?:with(?! cipher)\\s+(?P<with>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+id|\\s+for|\\s+via|;)) in from unknown HELO ?192.168.0.100? salimma1@212.18.241.211 with plain by smtp.mail.vip.sc5.yahoo.com with SMTP; 10 Oct 2002 10:30:25 -0000\n",
      "More than one match found for (?:with(?! cipher)\\s+(?P<with>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+id|\\s+for|\\s+via|;)) in from p977.as2.cra.dublin.eircom.net HELO mfrenchw2k mfrench42@159.134.179.209 with login by smtp.mail.vip.sc5.yahoo.com with SMTP; 22 Aug 2002 22:02:25 -0000\n",
      "More than one match found for (?:id\\s+(?P<id>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+with(?! cipher)|\\s+for|\\s+via|;)) in from chassid 4.65.20.230 by out001.verizon.net InterMail vM.5.01.05.09 201-253-122-126-109-20020611 with ESMTP id <20021005011206.OGMC3265.out001.verizon.net@chassid> for <rpm-list@freshrpms.net>; Fri, 4 Oct 2002 20:12:06 -0500\n",
      "More than one match found for (?:id\\s+(?P<id>.+?)(?:\\s*[(]?envelope-from|\\s*[(]?envelope-sender|\\s+from|\\s+by|\\s+with(?! cipher)|\\s+for|\\s+via|;)) in from chassid 4.65.20.230 by out002.verizon.net InterMail vM.5.01.05.09 201-253-122-126-109-20020611 with ESMTP id <20021005195843.LSGV2867.out002.verizon.net@chassid> for <rpm-list@freshrpms.net>; Sat, 5 Oct 2002 14:58:43 -0500\n",
      "Unable to match any clauses in qfscan 0.5. spam=2/0/0/0 remoteip=172.16.0.211 : 29 Aug 2002 03:27:47 -0000\n"
     ]
    }
   ],
   "source": [
    "#Giving filepaths for the data\n",
    "dat_path = \"datasets/spamham\"\n",
    "spam_path = dat_path + '/easy_spam'\n",
    "ham_path = dat_path + '/easy_ham'\n",
    "\n",
    "len_spam = len(next(os.walk(spam_path))[2])\n",
    "len_ham = len(next(os.walk(ham_path))[2])\n",
    "\n",
    "ar_dict = [(spam_path, len_spam), (ham_path, len_ham)] \n",
    "sl = math.ceil(0.8 * len_spam)\n",
    "hl = math.ceil(0.8 * len_ham)\n",
    "\n",
    "y_train = [0] * sl + [1] * hl\n",
    "y_test = [0] * (len_spam - sl) + [1] * (len_ham - hl)\n",
    "\n",
    "list_mails = []\n",
    "\n",
    "#Reading emails and transforming them in a list of words and combining them for all mails\n",
    "for sp, num in ar_dict:\n",
    "    print('preparing data for ', sp)\n",
    "    for file_num in range(num):\n",
    "        data_mail = sp + '/mail_' + str(file_num)\n",
    "        mail = mp.parse_from_file(data_mail)\n",
    "        list_mails.append(mail)\n",
    "        \n",
    "\n",
    "#dividing test and train mails from all mails(list_mails). 20 percent is test in each ham and spam\n",
    "train_mails = list_mails[:sl] + list_mails[len_spam: len_spam + hl]\n",
    "test_mails = list_mails[sl:len_spam] + list_mails[len_spam + hl:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "making features, inside training\n",
      "preparing feature vectors... \n",
      "Preparing data...\n",
      "making features, inside testing\n",
      "preparing feature vectors... \n"
     ]
    }
   ],
   "source": [
    "MyPipeline = Pipeline([\n",
    "    ('mailstolistwords', preparing_data()),\n",
    "    ('making_features', making_features()),\n",
    "    ('making_feature_vectors', making_fvectors())\n",
    "])\n",
    "\n",
    "df_train = MyPipeline.fit_transform(train_mails)\n",
    "df_test = MyPipeline.fit_transform(test_mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42) \n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_estimators': randint(10, 50),\n",
    "     'max_features': randint(1, df_train.shape[1]),\n",
    "}\n",
    "\n",
    "newgridsearch = RandomizedSearchCV(clf, param_distributions = params, n_iter=20, cv = 5, scoring='accuracy')\n",
    "r_grid_search = newgridsearch.fit(df_train, y_train)\n",
    "\n",
    "final_model = r_grid_search.best_estimator_\n",
    "frst_scrs = cross_val_score(final_model, df_train, y_train, cv = 3, scoring = \"accuracy\")\n",
    "print('Training scores for best estimator ')\n",
    "frst_scrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores for best estimator \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.98773006, 0.9803198 , 0.97416974])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = final_model.predict(df_test)\n",
    "accuracy = sum(1 for x,y in zip(predictions,y_test) if x == y) / len(predictions)\n",
    "print('Testing scores for best estimator ')\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scores for best estimator \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.978688524590164"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "ps = precision_score(y_test, predictions)\n",
    "rs = recall_score (y_test, predictions)\n",
    "print('Precision score: ', ps)\n",
    "print('Recall score: ', rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
