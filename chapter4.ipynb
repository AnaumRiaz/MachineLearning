{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "plt.scatter(X,y)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)       #$sign just making it itlaics\n",
    "\n",
    "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding batch descent vs Stochastic gradient descend\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "    \n",
    "#stochastic from scratch\n",
    "theta_path_sgd = []\n",
    "m = len(X_b)\n",
    "np.random.seed(42)\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        if epoch == 0 and i < 20:                    # not shown in the book\n",
    "            y_predict = X_new_b.dot(theta)           # not shown\n",
    "            style = \"b-\" if i > 0 else \"r--\"         # not shown\n",
    "            plt.plot(X_new, y_predict, style)        # not shown\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)                 # not shown\n",
    "\n",
    "plt.plot(X, y, \"b.\")                                 # not shown\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)                     # not shown\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)           # not shown\n",
    "plt.axis([0, 2, 0, 15])                              # not shown                          # not shown\n",
    "plt.show()  \n",
    "\n",
    "#-----------builtin for stochastic\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.intercept_, sgd_reg.coef_\n",
    "\n",
    "#-------mini batch descent\n",
    "\n",
    "\n",
    "theta_path_mgd = []\n",
    "\n",
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = 0\n",
    "for epoch in range(n_iterations):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        t += 1\n",
    "        xi = X_b_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_mgd.append(theta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generating quadratic tdata for polynomial regression\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "plt.scatter(X, y)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting the regressor curve of different models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting learing curves (first is underfitting because rmse is high and train/test is same)\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def plot_learning_curves(model, X, y):\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "#     train_errors, val_errors = [], []\n",
    "#     for m in range(1, len(X_train)):\n",
    "#         model.fit(X_train[:m], y_train[:m])\n",
    "#         y_train_predict = model.predict(X_train[:m])\n",
    "#         y_val_predict = model.predict(X_val)\n",
    "#         train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "#         val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "#     plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "#     plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "#     plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n",
    "#     plt.xlabel(\"Training set size\", fontsize=14) # not shown\n",
    "#     plt.ylabel(\"RMSE\", fontsize=14)              # not shown\n",
    "\n",
    "# lin_reg = LinearRegression()\n",
    "# plot_learning_curves(lin_reg, X, y)\n",
    "# plt.axis([0, 80, 0, 3])                         # not shown in the book\n",
    "# plt.show()  \n",
    "\n",
    "\n",
    "#overfitting \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "plt.axis([0, 80, 0, 3])           # not shown\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularizations\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "# ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
    "# ridge_reg.fit(X, y)\n",
    "# ridge_reg.predict([[1.5]])\n",
    "\n",
    "# d_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=\"l2\", random_state=42)\n",
    "# sgd_reg.fit(X, y.ravel())\n",
    "# sgd_reg.predict([[1.5]])\n",
    "\n",
    "# #lasso\n",
    "\n",
    "# from sklearn.linear_model import Lasso\n",
    "# lasso_reg = Lasso(alpha=0.1)\n",
    "# lasso_reg.fit(X, y)\n",
    "# lasso_reg.predict([[1.5]])\n",
    "\n",
    "\n",
    "# d_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=\"l1\", random_state=42)\n",
    "# sgd_reg.fit(X, y.ravel())\n",
    "# sgd_reg.predict([[1.5]])\n",
    " \n",
    "# #Elastic net\n",
    "# from sklearn.linear_model import ElasticNet\n",
    "# elastic_net = ElasticNet(alpha = 0.1, l1_ratio = 0.5)\n",
    "# elastic_net.fit(X,y)\n",
    "# elastic_net.predict([[1.5]])\n",
    "\n",
    "#Early stopping\n",
    "from sklearn.base import clone\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, penalty=None,  eta0=0.0005, warm_start=True,learning_rate=\"constant\",\n",
    "                       random_state=42)\n",
    "\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "\n",
    "n_epochs = 1000\n",
    "train_errors, val_errors = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
    "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    \n",
    "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
    "    val_errors.append(val_error)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)\n",
    "\n",
    "#best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
    "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting sigmoid\n",
    "\n",
    "t = np.linspace(-10, 10, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot([-10, 10], [0, 0], \"k-\")\n",
    "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-10, 10], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\", fontsize=20)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#playing with iris dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision boundaries\n",
    "X =  iris['data'][:, 3:]\n",
    "y = (iris['target'] == 2).astype(int)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver=\"liblinear\",random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")\n",
    "\n",
    "#to see where the line is for the two classes\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "log_reg.predict([[1.7], [1.5]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4c6bf65507f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msoftmax_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multinomial\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msoftmax_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "#softmax regression\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)\n",
    "\n",
    "softmax_reg.predict_proba([[5,2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_error < min_val_error\n",
      "Training 1 1.238415601320209\n",
      "validation 1 1.3311828530753462\n",
      "Training 2 1.0907866085391469\n",
      "validation 2 1.1483093347688482\n",
      "Training 3 1.0590579673088991\n",
      "validation 3 1.1046972155253536\n",
      "Training 4 1.0484323614849465\n",
      "validation 4 1.093273566232311\n",
      "Training 5 1.0412123219979261\n",
      "validation 5 1.0867060492599063\n",
      "Training 6 1.0351043783138425\n",
      "validation 6 1.080847978245178\n",
      "Training 7 1.029472788661741\n",
      "validation 7 1.0750350791883\n",
      "Training 8 1.0240772624648897\n",
      "validation 8 1.0691688473038394\n",
      "Training 9 1.018817498092705\n",
      "validation 9 1.0632526376468465\n",
      "Training 10 1.013649084584223\n",
      "validation 10 1.0573081997356089\n",
      "Training 11 1.008551496641026\n",
      "validation 11 1.051357592434968\n",
      "Training 12 1.0035148926690582\n",
      "validation 12 1.0454190144300586\n",
      "Training 13 0.9985343619024053\n",
      "validation 13 1.0395063621214236\n",
      "Training 14 0.9936073192419695\n",
      "validation 14 1.0336298303972173\n",
      "Training 15 0.9887322929088864\n",
      "validation 15 1.0277967087269677\n",
      "Training 16 0.9839083491917767\n",
      "validation 16 1.022012103114967\n",
      "Training 17 0.9791348157027945\n",
      "validation 17 1.0162795167268044\n",
      "Training 18 0.9744111469673464\n",
      "validation 18 1.0106012919367446\n",
      "Training 19 0.9697368587496832\n",
      "validation 19 1.0049789371999922\n",
      "Training 20 0.9651114958781364\n",
      "validation 20 0.9994133647392917\n",
      "Training 21 0.9605346165017543\n",
      "validation 21 0.9939050616555458\n",
      "Training 22 0.9560057844393506\n",
      "validation 22 0.9884542123680716\n",
      "Training 23 0.9515245655216249\n",
      "validation 23 0.9830607859182554\n",
      "Training 24 0.9470905259027457\n",
      "validation 24 0.9777245981022279\n",
      "Training 25 0.9427032313401196\n",
      "validation 25 0.9724453556572554\n",
      "Training 26 0.9383622469466357\n",
      "validation 26 0.9672226876893077\n",
      "Training 27 0.9340671371703695\n",
      "validation 27 0.9620561680441888\n",
      "Training 28 0.9298174658811443\n",
      "validation 28 0.9569453312548926\n",
      "Training 29 0.9256127965050343\n",
      "validation 29 0.9518896839330838\n",
      "Training 30 0.9214526921784173\n",
      "validation 30 0.9468887129284105\n",
      "Training 31 0.9173367159081536\n",
      "validation 31 0.9419418911932459\n",
      "Training 32 0.9132644307317689\n",
      "validation 32 0.9370486820169569\n",
      "Training 33 0.9092353998750017\n",
      "validation 33 0.9322085421002179\n",
      "Training 34 0.9052491869056878\n",
      "validation 34 0.9274209238029376\n",
      "Training 35 0.9013053558836794\n",
      "validation 35 0.9226852768024206\n",
      "Training 36 0.8974034715067796\n",
      "validation 36 0.9180010493297786\n",
      "Training 37 0.8935430992527746\n",
      "validation 37 0.9133676891039851\n",
      "Training 38 0.8897238055176521\n",
      "validation 38 0.9087846440485039\n",
      "Training 39 0.8859451577500843\n",
      "validation 39 0.9042513628509639\n",
      "Training 40 0.8822067245822156\n",
      "validation 40 0.8997672954089867\n",
      "Training 41 0.8785080759567928\n",
      "validation 41 0.8953318931929309\n",
      "Training 42 0.8748487832506282\n",
      "validation 42 0.8909446095475139\n",
      "Training 43 0.8712284193944035\n",
      "validation 43 0.8866048999480365\n",
      "Training 44 0.8676465589887946\n",
      "validation 44 0.8823122222224545\n",
      "Training 45 0.8641027784168983\n",
      "validation 45 0.8780660367473679\n",
      "Training 46 0.8605966559529501\n",
      "validation 46 0.8738658066237233\n",
      "Training 47 0.8571277718673098\n",
      "validation 47 0.8697109978363968\n",
      "Training 48 0.8536957085277073\n",
      "validation 48 0.8656010794006653\n",
      "Training 49 0.8503000504967387\n",
      "validation 49 0.8615355234977423\n",
      "Training 50 0.8469403846256102\n",
      "validation 50 0.8575138056009595\n",
      "Training 51 0.8436163001441285\n",
      "validation 51 0.8535354045937471\n",
      "Training 52 0.8403273887469537\n",
      "validation 52 0.849599802880267\n",
      "Training 53 0.8370732446761178\n",
      "validation 53 0.8457064864893287\n",
      "Training 54 0.8338534647998391\n",
      "validation 54 0.8418549451720677\n",
      "Training 55 0.8306676486876474\n",
      "validation 55 0.8380446724937537\n",
      "Training 56 0.8275153986818574\n",
      "validation 56 0.8342751659200162\n",
      "Training 57 0.824396319965421\n",
      "validation 57 0.8305459268977217\n",
      "Training 58 0.8213100206262018\n",
      "validation 58 0.8268564609306962\n",
      "Training 59 0.8182561117177154\n",
      "validation 59 0.8232062776504646\n",
      "Training 60 0.8152342073163801\n",
      "validation 60 0.819594890882144\n",
      "Training 61 0.8122439245753379\n",
      "validation 61 0.8160218187056435\n",
      "Training 62 0.8092848837748974\n",
      "validation 62 0.8124865835122815\n",
      "Training 63 0.8063567083696572\n",
      "validation 63 0.808988712056952\n",
      "Training 64 0.8034590250323776\n",
      "validation 64 0.8055277355059565\n",
      "Training 65 0.8005914636946599\n",
      "validation 65 0.802103189480612\n",
      "Training 66 0.7977536575845046\n",
      "validation 66 0.7987146140967586\n",
      "Training 67 0.794945243260818\n",
      "validation 67 0.7953615540002726\n",
      "Training 68 0.7921658606449379\n",
      "validation 68 0.7920435583987053\n",
      "Training 69 0.7894151530492532\n",
      "validation 69 0.7887601810891577\n",
      "Training 70 0.786692767202989\n",
      "validation 70 0.7855109804825061\n",
      "Training 71 0.7839983532752389\n",
      "validation 71 0.7822955196240926\n",
      "Training 72 0.7813315648953153\n",
      "validation 72 0.7791133662109933\n",
      "Training 73 0.7786920591704982\n",
      "validation 73 0.7759640926059806\n",
      "Training 74 0.7760794967012618\n",
      "validation 74 0.7728472758482862\n",
      "Training 75 0.7734935415940517\n",
      "validation 75 0.7697624976612857\n",
      "Training 76 0.7709338614717007\n",
      "validation 76 0.76670934445721\n",
      "Training 77 0.7684001274815501\n",
      "validation 77 0.7636874073389954\n",
      "Training 78 0.765892014301363\n",
      "validation 78 0.760696282099388\n",
      "Training 79 0.7634092001431071\n",
      "validation 79 0.7577355692174009\n",
      "Training 80 0.7609513667546766\n",
      "validation 80 0.7548048738522416\n",
      "Training 81 0.7585181994196404\n",
      "validation 81 0.7519038058348086\n",
      "Training 82 0.7561093869550847\n",
      "validation 82 0.7490319796568662\n",
      "Training 83 0.7537246217076289\n",
      "validation 83 0.7461890144579948\n",
      "Training 84 0.7513635995476875\n",
      "validation 84 0.7433745340104247\n",
      "Training 85 0.7490260198620508\n",
      "validation 85 0.7405881667018435\n",
      "Training 86 0.7467115855448583\n",
      "validation 86 0.7378295455162824\n",
      "Training 87 0.744420002987033\n",
      "validation 87 0.735098308013172\n",
      "Training 88 0.7421509820642473\n",
      "validation 88 0.7323940963046589\n",
      "Training 89 0.7399042361234892\n",
      "validation 89 0.7297165570312775\n",
      "Training 90 0.737679481968294\n",
      "validation 90 0.7270653413360636\n",
      "Training 91 0.7354764398427088\n",
      "validation 91 0.7244401048371946\n",
      "Training 92 0.7332948334140487\n",
      "validation 92 0.7218405075992426\n",
      "Training 93 0.7311343897545177\n",
      "validation 93 0.7192662141031203\n",
      "Training 94 0.7289948393217404\n",
      "validation 94 0.7167168932147995\n",
      "Training 95 0.7268759159382765\n",
      "validation 95 0.7141922181528774\n",
      "Training 96 0.7247773567701689\n",
      "validation 96 0.7116918664550698\n",
      "Training 97 0.7226989023045823\n",
      "validation 97 0.7092155199436969\n",
      "Training 98 0.720640296326591\n",
      "validation 98 0.706762864690237\n",
      "Training 99 0.7186012858951601\n",
      "validation 99 0.7043335909790135\n",
      "Training 100 0.7165816213183832\n",
      "validation 100 0.7019273932700795\n",
      "Training 101 0.7145810561280137\n",
      "validation 101 0.6995439701613665\n"
     ]
    }
   ],
   "source": [
    "#question 12\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "\n",
    "def hotcoding_labels(labels):\n",
    "    nar = np.zeros((labels.shape[0], n_classes))\n",
    "    nar[np.arange(labels.shape[0]), labels] = 1\n",
    "    return nar\n",
    "\n",
    "def sfmax_func(array_of_scores):\n",
    "    array_exp = np.exp(array_of_scores)                         #shape 150, 3\n",
    "    ar_row_sum = np.sum(array_exp, axis = 1, keepdims = True)   #shape = 150, 1\n",
    "    pk = array_exp / ar_row_sum\n",
    "    return pk\n",
    "\n",
    "\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y_all = iris[\"target\"]\n",
    "eta = 0.1 \n",
    "\n",
    "num_samples = X.shape[0]\n",
    "val_num = np.ceil(0.2 * num_samples).astype(int)\n",
    "train_num = np.ceil(0.6 * num_samples).astype(int)\n",
    "\n",
    "shuffle_indices = np.random.permutation(X.shape[0])\n",
    "X1 = X[shuffle_indices[:train_num]]\n",
    "X_val = X[shuffle_indices[train_num: train_num + val_num]]\n",
    "X_test = X[shuffle_indices[train_num + val_num :]]\n",
    "y = y_all[shuffle_indices[:train_num]]\n",
    "y_val = y_all[shuffle_indices[train_num: train_num + val_num]]\n",
    "y_test = y_all[shuffle_indices[train_num + val_num :]]\n",
    "\n",
    "ep = 1e-10\n",
    "m = y.shape[0]\n",
    "X_train = np.c_[np.ones((m, 1)), X1]  # add 1 to each instance at the beginning\n",
    "X_val = np.c_[np.ones((X_val.shape[0],1)), X_val]\n",
    "X_test = np.c_[np.ones((X_test.shape[0],1)), X_test]\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "n_feats = X_train.shape[1]\n",
    "\n",
    "#we need to change the target label in a way that is understood by the loss equation\n",
    "labels_encoded = hotcoding_labels(y)\n",
    "labels_encoded_val = hotcoding_labels(y_val)\n",
    "\n",
    "thetas = np.random.rand(n_feats ,n_classes)\n",
    "\n",
    "condition_stop = 100 \n",
    "count = 0\n",
    "ite = 0\n",
    "min_val_error = float('inf')\n",
    "num_iterations = 5000\n",
    "\n",
    "while(count < condition_stop and ite < num_iterations):\n",
    "\n",
    "    ite = ite + 1\n",
    "    #print('iteration num: ',ite)\n",
    "    sm_score_train = np.dot(X_train, thetas)\n",
    "    pk_train = sfmax_func(sm_score_train)\n",
    "\n",
    "    sm_score_val = np.dot(X_val, thetas)\n",
    "    pk_val = sfmax_func(sm_score_val)\n",
    "    \n",
    "    #val_out = np.argmax(pk_val, axis = 1)\n",
    "    #val_error = mean_squared_error(y_val, val_out)\n",
    "    #print('val_error', val_error)\n",
    "    jtheta_val = -np.mean(np.sum(labels_encoded_val * np.log(pk_val + ep), axis=1))\n",
    "    #print('validation array out: ', val_out)\n",
    "    \n",
    "    if jtheta_val < min_val_error:\n",
    "        print('val_error < min_val_error')\n",
    "        min_val_error = val_error\n",
    "        min_thetas = thetas\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "    \n",
    "    #print('count: ',count)\n",
    "    Jtheta = -np.mean(np.sum(labels_encoded * np.log(pk_train + ep), axis=1))\n",
    "    print('Training',ite, Jtheta)\n",
    "    print('validation',ite, jtheta_val)\n",
    "    \n",
    "    error = pk_train - labels_encoded\n",
    "    newar = []\n",
    "    for i in range(n_classes):\n",
    "        er = error[:, i].reshape(-1, 1)\n",
    "        toappend = np.mean((X_train * er), axis = 0)\n",
    "        newar.append(toappend)\n",
    "        gradients = np.array(newar).T\n",
    "\n",
    "    #gradeints = gradients.T   #becuase my way resulted in classes in rows as in 1st row first class theta second row second class theta and so on. taking transpose makes it how it is \n",
    "    thetas = thetas - eta * gradients\n",
    "\n",
    "#loop ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets see the accuracy on training and validations sets\n",
    "\n",
    "thetas = min_thetas\n",
    "sm_score_train = np.dot(X_train, thetas)\n",
    "pk_train = sfmax_func(sm_score_train)\n",
    "y_pred = np.argmax(pk_train, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7222222222222222"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nar = np.zeros((m, n_classes))\n",
    "nar[np.arange(m), y] = 1\n",
    "nar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
