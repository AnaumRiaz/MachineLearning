{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the transformers for the Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#Tackling Ticket to make TktNum and TktPre and dropping ticket column\n",
    "class Tackling_Ticket(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x2, y=None):\n",
    "        return self\n",
    "    def transform(self, x2, y=None):\n",
    "        #appending an 'X' for tickets with no prefix and making two new columns\n",
    "        x2['Ticket'] = x2['Ticket'].apply(lambda x: x.replace(' ','', 1) if len(x.split()) == 3 else x)\n",
    "        x2['Ticket'] = x2['Ticket'].apply(lambda x: ('X ' + x ) if len(x.split()) == 1 else x)\n",
    "        x2[['TktPre', 'TktNum']] = x2.Ticket.str.split(' ', expand = True)\n",
    "        x2[\"TktNum\"] = x2[\"TktNum\"].replace('LINE', 0)\n",
    "        x2.drop(['Ticket'], axis = 1, inplace = True)\n",
    "        return x2\n",
    "\n",
    "#Divind TktPre in categories so we can use dummies on them\n",
    "class categorizing_TktPre(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x2, y=None):\n",
    "        return self\n",
    "    def transform(self, x2, y=None):\n",
    "       # print(\"inside\", x2)\n",
    "        a = x2.TktPre.value_counts().index\n",
    "        cat1, cat2, cat3 = a[0], a[1:5], a[5:]\n",
    "        x2[\"TktPre\"] = x2[\"TktPre\"].apply(lambda x: (x in cat1 and 'cat1') or (x in cat2 and 'cat2') or 'cat3')\n",
    "        return x2\n",
    "\n",
    "#making catgories for fare \n",
    "class categorizing_Continuous(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x2, y=None):\n",
    "        return self\n",
    "    def transform(self, x2, y=None):\n",
    "        #labels = np.arange(1,21)\n",
    "        x2 = pd.DataFrame(x2, columns = num_attribs)\n",
    "        x2['Fare']= x2['Age'].fillna(x2['Age'].mean())\n",
    "        x2[\"Fare\"] = pd.qcut(x2.Fare, q = 5, labels = False, duplicates = 'drop')\n",
    "        return x2\n",
    "\n",
    "def myfun(row):\n",
    "    if (math.isnan(row['Age']) and row['Survived'] == 1):\n",
    "        return 0.75\n",
    "    elif (math.isnan(row['Age']) and row['Survived'] == 0):\n",
    "        return 70\n",
    "    else:\n",
    "        return row['Age']  \n",
    "    \n",
    "#Filling in the age valeus for nan. age 70 has 0 survival rate and 0.75 has 1\n",
    "class Age_impute(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x2, y=None):\n",
    "        return self\n",
    "    def transform(self, x2, y=None):\n",
    "        if (x2.shape[0] == 891 ):\n",
    "            x2['Survived'] = tit_data2[\"Survived\"]\n",
    "            #print(\"YES\")\n",
    "            x2['Age'] = x2.apply(lambda row: myfun(row), axis = 1)\n",
    "            #print(x2.shape)\n",
    "            x2.drop('Survived', axis = 1, inplace = True)\n",
    "            return x2\n",
    "        else:\n",
    "            x2 = x2.fillna(x2.median())\n",
    "            return x2\n",
    "\n",
    "class Age_impute_closestRows(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x2, y=None):\n",
    "        return self\n",
    "    def transform(self, dataset, y=None):\n",
    "            index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n",
    "            for i in index_NaN_age :\n",
    "                age_med = dataset[\"Age\"].median()\n",
    "                age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n",
    "                if not np.isnan(age_pred) :\n",
    "                    dataset['Age'].iloc[i] = age_pred\n",
    "                else :\n",
    "                    dataset['Age'].iloc[i] = age_med\n",
    "            return dataset\n",
    "\n",
    "        \n",
    "#Taking first alphabet of the cabin and 'M' for empty columns\n",
    "class Prep_Cabin(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x2, y=None):\n",
    "        return self\n",
    "    def transform(self, x2, y=None):\n",
    "        key = lambda x: 'M' if pd.isnull(x['Cabin']) else x['Cabin'][0]\n",
    "        x2['Cabin'] = x2.apply(key, axis = 1)\n",
    "        return x2\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 23)\n",
      "(418, 23)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "\n",
    "\n",
    "TITANIC_TRAINING_PATH = \"datasets/Titanic/train.csv\"\n",
    "TITANIC_TESTING_PATH = \"datasets/Titanic/test.csv\"\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_titanic_data(titanic_path = TITANIC_TRAINING_PATH):\n",
    "    return pd.read_csv(titanic_path)\n",
    "\n",
    "tit_test_data = load_titanic_data(TITANIC_TESTING_PATH)\n",
    "tit_data = load_titanic_data()\n",
    "tit_data2 = tit_data.copy()\n",
    "\n",
    "y_train = (tit_data2[\"Survived\"]).values\n",
    "\n",
    "tit_data2_arr = tit_data2.values\n",
    "#randomizing\n",
    "shuffle_index = np.random.permutation(891)    #0 to 890\n",
    "tit_data2_arr, y_train = tit_data2_arr[shuffle_index], y_train[shuffle_index]\n",
    "tit_data2 = pd.DataFrame(tit_data2_arr, columns = tit_data2.columns)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#DIviding features in numerical and categorical for Pipelines\n",
    "num_attribs = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "cat_attribs = ['Sex', 'Embarked', 'Ticket', 'Cabin']\n",
    "\n",
    "dummies_columns = ['Sex_F','Sex_M', 'Em_C', 'Em_Q', 'Em_S',  'Cab_A', 'Cab_B','Cab_C','Cab_D','Cab_E' ,'Cab_F' ,'Cab_G' ,'Cab_M' ,'Cab_T' ,'Tkt_cat1', 'Tkt_cat2', 'Tkt_cat3' ,'TktNum']\n",
    "columns_prepared = num_attribs + dummies_columns\n",
    "\n",
    "\n",
    "\n",
    "#making dummies of the features\n",
    "#using individual columns for dummies because thats the only way to ensure dummies remain same over test and train\n",
    "class making_dummies(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x2, y=None):\n",
    "        return self\n",
    "    def transform(self, x2, y=None):\n",
    "        df_dum = list()\n",
    "        string_feats = x2[['Sex', 'Embarked', 'Cabin', 'TktPre']]\n",
    "        \n",
    "        cats = {string_feats.columns[0]: ['female', 'male'], string_feats.columns[1]: ['C', 'Q', 'S'], string_feats.columns[2]: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T'], string_feats.columns[3]: ['cat1', 'cat2', 'cat3'] }\n",
    "        \n",
    "        for column in string_feats:\n",
    "            string_feats[column] = string_feats[column].astype('category', categories = cats[column])\n",
    "            data = pd.get_dummies(string_feats[column])\n",
    "            data_parse = data.values.shape[1]\n",
    "            for i in range(data_parse):\n",
    "                lst = [item[i] for item in data.values]\n",
    "                df_dum.append(lst) \n",
    "        dumns = [item for items in cats.values() for item in items]\n",
    "        df_dum = pd.DataFrame( np.array(df_dum).T, columns = dumns)\n",
    "        df_dum = pd.concat([df_dum, x2['TktNum']], axis = 1)\n",
    "        \n",
    "        return df_dum\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', Age_impute()),\n",
    "        #('imputer', SimpleImputer(strategy = 'mean')),\n",
    "        ('categorizing_continuous', categorizing_Continuous()),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('Tackling_Ticket', Tackling_Ticket()),\n",
    "        ('categorizing TktPre', categorizing_TktPre()),\n",
    "        ('Cabin_prep', Prep_Cabin()),\n",
    "        ('Getting_dummies', making_dummies() )\n",
    "    ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "\n",
    "data_prepared = full_pipeline.fit_transform(tit_data2)\n",
    "X_train = pd.DataFrame(data_prepared, columns = columns_prepared)\n",
    "print(X_train.shape)\n",
    "\n",
    "testing_data_prepared = full_pipeline.fit_transform(tit_test_data)\n",
    "X_test = pd.DataFrame(testing_data_prepared, columns = columns_prepared)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing outliers\n",
    "\n",
    "from collections import Counter\n",
    "print(\"training set before removing outliers \", X_train.shape)\n",
    "\n",
    "#removing otliers from the data\n",
    "def remove_outlier_rows(f_name_array, given_number, data_name):\n",
    "    outlier_cols = []\n",
    "    \n",
    "    for one_col in f_name_array:\n",
    "        P1 = np.percentile(data_name[one_col], 25)\n",
    "        P2 = np.percentile(data_name[one_col], 75)\n",
    "        IQR = P2 - P1\n",
    "        outlier_step = 1.5 * IQR\n",
    "       \n",
    "        indices_outlier_rows = data_name[(data_name[one_col] > P2 + outlier_step) | (data_name[one_col] < P1 - outlier_step)].index\n",
    "        #print(X_train[(X_train[one_col] > P2 + IQR) | (X_train[one_col] < P1 - IQR)])\n",
    "        outlier_cols.extend(indices_outlier_rows)\n",
    "       \n",
    "    check = Counter(outlier_cols)\n",
    "    \n",
    "    rows_to_remove = [k for k,v in check.items() if v > given_number]\n",
    "    \n",
    "    print([rows_to_remove])\n",
    "    data_name.drop(rows_to_remove, inplace = True)\n",
    "    return rows_to_remove\n",
    "\n",
    "rows_removed_train = remove_outlier_rows([\"Age\", \"Parch\", \"SibSp\", \"Fare\"], 2, X_train)\n",
    "\n",
    "y_train = np.delete(y_train, rows_removed_train, 0)\n",
    "\n",
    "print(\"training set after removing outliers \",X_train.shape)\n",
    "print(\"training labels shape \", y_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.61391694725027"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "params = {\n",
    "    'n_estimators': randint(50, 100),\n",
    "     'max_features': randint(1, 24),\n",
    "     'oob_score' : [True],\n",
    "     'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "newgridsearch = RandomizedSearchCV(forest_clf, param_distributions = params, n_iter=20, cv = 5, scoring='accuracy')\n",
    "r_grid_search = newgridsearch.fit(X_train, y_train)\n",
    "\n",
    "final_model = newgridsearch.best_estimator_\n",
    "frst_scrs = cross_val_score(final_model, X_train, y_train, cv = 3, scoring = \"accuracy\")\n",
    "\n",
    "frst_scrs.mean()*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running on the test_Data\n",
    "final_model = newgridsearch.best_estimator_\n",
    "letsee = final_model.predict(X_test)\n",
    "letsee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata = np.vstack((tit_test_data.PassengerId, letsee)).ravel('F')\n",
    "fdata = fdata.reshape(-1,2)\n",
    "results_final = pd.DataFrame(fdata)\n",
    "results_final\n",
    "file_name = 'testing.csv'\n",
    "results_final.to_csv(file_name, header = ['PassengerId', 'Survived'], index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_bar(attribute):\n",
    "    X_train['Survived'] = pd.Series(y_train)\n",
    "    survivability = X_train.groupby([attribute])[\"Survived\"].mean()\n",
    "    values = X_train.groupby([attribute]).groups.keys()\n",
    "    plt.bar(values, survivability)\n",
    "    plt.ylabel('Survivability')\n",
    "    plt.xlabel(attribute)\n",
    "making_bar('Cabin')\n",
    "\n",
    "#plotting Cabin\n",
    "key = lambda x: 'M' if pd.isnull(x['Cabin']) else x['Cabin'][0]\n",
    "a = tit_test_data.apply(key, axis = 1)\n",
    "\n",
    "a1 = a.value_counts().index\n",
    "a2 = a.value_counts().values\n",
    "\n",
    "plt.bar(a1, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
